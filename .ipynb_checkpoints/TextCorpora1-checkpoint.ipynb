{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WWC - Accessing Text Corpora and Lexical Resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUTENBERG CORPUS\n",
    "\n",
    "NLTK includes a selection of texts from the Project Gutenberg Electronic Text Archive, which contains some 25,000 free electronic books hosted as *http://www.gutenberg.org*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the first text a short name, and find out how many words it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some concordancing in Emma's text and find the context for \"surprize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emmaText = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 37 matches:\n",
      "er father , was sometimes taken by surprize at his being still able to pity ` \n",
      "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
      "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
      "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
      "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
      "father was quite taken up with the surprize of so sudden a journey , and his f\n",
      "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
      "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
      "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
      "talking aunt had taken me quite by surprize , it must have been the death of m\n",
      "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
      " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
      "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
      " to her song took her agreeably by surprize -- a second , slightly but correct\n",
      "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
      "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
      "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
      "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
      " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
      " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
      "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
      "; and Emma could imagine with what surprize and mortification she must be retu\n",
      "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
      " . It is impossible to express our surprize . He came to speak to his father o\n",
      "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
     ]
    }
   ],
   "source": [
    "emmaText.concordance(\"surprize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **words()** function of the gutenberg object in NLTK's corpus package provides another version of the **import** statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a program to display general information about the texts by looping over all the values of **fileid** corresponding to the gutenberg files identifiers listed earlier and then computing statistics for each text.\n",
    "1. Average word length\n",
    "2. Average sentence length\n",
    "3. Average number of times each vocabulary item appears in the text on average (lexical diversity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 24 26 austen-emma.txt\n",
      "4 26 16 austen-persuasion.txt\n",
      "4 28 22 austen-sense.txt\n",
      "4 33 79 bible-kjv.txt\n",
      "4 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 17 12 burgess-busterbrown.txt\n",
      "4 20 12 carroll-alice.txt\n",
      "4 20 11 chesterton-ball.txt\n",
      "4 22 11 chesterton-brown.txt\n",
      "4 18 10 chesterton-thursday.txt\n",
      "4 20 24 edgeworth-parents.txt\n",
      "4 25 15 melville-moby_dick.txt\n",
      "4 52 10 milton-paradise.txt\n",
      "4 11 8 shakespeare-caesar.txt\n",
      "4 12 7 shakespeare-hamlet.txt\n",
      "4 12 6 shakespeare-macbeth.txt\n",
      "4 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid)) # raw() returns the number of characters. Not split into tokens\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))# sents() divides the text into sentences\n",
    "    num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
    "    print(int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sents( ) divides the text up into sentences, where each sentence is a list of words. Check the next example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Actus', 'Primus', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'night', ',', 'and', 'better', 'health', 'Attend', 'his', 'Maiesty']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences[1037]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find the longest sentence in Macbeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longest_len = max([len(s) for s in macbeth_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Doubtfull',\n",
       "  'it',\n",
       "  'stood',\n",
       "  ',',\n",
       "  'As',\n",
       "  'two',\n",
       "  'spent',\n",
       "  'Swimmers',\n",
       "  ',',\n",
       "  'that',\n",
       "  'doe',\n",
       "  'cling',\n",
       "  'together',\n",
       "  ',',\n",
       "  'And',\n",
       "  'choake',\n",
       "  'their',\n",
       "  'Art',\n",
       "  ':',\n",
       "  'The',\n",
       "  'mercilesse',\n",
       "  'Macdonwald',\n",
       "  '(',\n",
       "  'Worthie',\n",
       "  'to',\n",
       "  'be',\n",
       "  'a',\n",
       "  'Rebell',\n",
       "  ',',\n",
       "  'for',\n",
       "  'to',\n",
       "  'that',\n",
       "  'The',\n",
       "  'multiplying',\n",
       "  'Villanies',\n",
       "  'of',\n",
       "  'Nature',\n",
       "  'Doe',\n",
       "  'swarme',\n",
       "  'vpon',\n",
       "  'him',\n",
       "  ')',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Westerne',\n",
       "  'Isles',\n",
       "  'Of',\n",
       "  'Kernes',\n",
       "  'and',\n",
       "  'Gallowgrosses',\n",
       "  'is',\n",
       "  'supply',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  ',',\n",
       "  'And',\n",
       "  'Fortune',\n",
       "  'on',\n",
       "  'his',\n",
       "  'damned',\n",
       "  'Quarry',\n",
       "  'smiling',\n",
       "  ',',\n",
       "  'Shew',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'like',\n",
       "  'a',\n",
       "  'Rebells',\n",
       "  'Whore',\n",
       "  ':',\n",
       "  'but',\n",
       "  'all',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'too',\n",
       "  'weake',\n",
       "  ':',\n",
       "  'For',\n",
       "  'braue',\n",
       "  'Macbeth',\n",
       "  '(',\n",
       "  'well',\n",
       "  'hee',\n",
       "  'deserues',\n",
       "  'that',\n",
       "  'Name',\n",
       "  ')',\n",
       "  'Disdayning',\n",
       "  'Fortune',\n",
       "  ',',\n",
       "  'with',\n",
       "  'his',\n",
       "  'brandisht',\n",
       "  'Steele',\n",
       "  ',',\n",
       "  'Which',\n",
       "  'smoak',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'with',\n",
       "  'bloody',\n",
       "  'execution',\n",
       "  '(',\n",
       "  'Like',\n",
       "  'Valours',\n",
       "  'Minion',\n",
       "  ')',\n",
       "  'caru',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'out',\n",
       "  'his',\n",
       "  'passage',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'hee',\n",
       "  'fac',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'the',\n",
       "  'Slaue',\n",
       "  ':',\n",
       "  'Which',\n",
       "  'neu',\n",
       "  \"'\",\n",
       "  'r',\n",
       "  'shooke',\n",
       "  'hands',\n",
       "  ',',\n",
       "  'nor',\n",
       "  'bad',\n",
       "  'farwell',\n",
       "  'to',\n",
       "  'him',\n",
       "  ',',\n",
       "  'Till',\n",
       "  'he',\n",
       "  'vnseam',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'him',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Naue',\n",
       "  'toth',\n",
       "  \"'\",\n",
       "  'Chops',\n",
       "  ',',\n",
       "  'And',\n",
       "  'fix',\n",
       "  \"'\",\n",
       "  'd',\n",
       "  'his',\n",
       "  'Head',\n",
       "  'vpon',\n",
       "  'our',\n",
       "  'Battlements']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in macbeth_sentences if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB and CHAT Text\n",
    "\n",
    "NLTK contains a small collections of web text which includes content from a Firefox discuss forum, conversations overhead in New York, the movie script of Pirates of the Caribbean, personal advertisements, and wine reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb\n"
     ]
    }
   ],
   "source": [
    "for fileid in webtext.fileids():\n",
    "    print(fileid, webtext.raw(fileid)[:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a corpus of instant messaging chat sessions, originally collected by the Naval Postgraduates School for research on automatic detection of Internet pedators. \n",
    "The corpus contains over 10,000 anonymized posts.\n",
    "The corpus is organized in 15 files collected in a specific date for an age-specific chatroom (teens, 20s, 30s,...)\n",
    "The filename contains the date, chatroom and number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'hot',\n",
       " 'pics',\n",
       " 'of',\n",
       " 'a',\n",
       " 'female',\n",
       " ',',\n",
       " 'I',\n",
       " 'can',\n",
       " 'look',\n",
       " 'in',\n",
       " 'a',\n",
       " 'mirror',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatroom[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BROWN CORPUS\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "This corpus contains text from 500 sources, and the sources have been categorized by genre (news, editorial, romance, and so on.)\n",
    "For a complete list see *http://icame.uib.no/brown/bcm-los.html*\n",
    "\n",
    "You can access the corpus as a list of words or a list of sentences, or by category.\n",
    "\n",
    "The Brown Corpus is a convenient resource for studying systematic differences between genres such a stylistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does',\n",
       " 'our',\n",
       " 'society',\n",
       " 'have',\n",
       " 'a',\n",
       " 'runaway',\n",
       " ',',\n",
       " 'uncontrollable',\n",
       " 'growth',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'which',\n",
       " 'may',\n",
       " 'end',\n",
       " 'our']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(fileids=['cg22'])[:15] # 'cg22' fileid specifies belles lettres genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'jury',\n",
       " 'further',\n",
       " 'said',\n",
       " 'in',\n",
       " 'term-end',\n",
       " 'presentments',\n",
       " 'that',\n",
       " 'the',\n",
       " 'City',\n",
       " 'Executive',\n",
       " 'Committee',\n",
       " ',',\n",
       " 'which',\n",
       " 'had',\n",
       " 'over-all',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'election',\n",
       " ',',\n",
       " '``',\n",
       " 'deserves',\n",
       " 'the',\n",
       " 'praise',\n",
       " 'and',\n",
       " 'thanks',\n",
       " 'of',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'Atlanta',\n",
       " \"''\",\n",
       " 'for',\n",
       " 'the',\n",
       " 'manner',\n",
       " 'in',\n",
       " 'which',\n",
       " 'the',\n",
       " 'election',\n",
       " 'was',\n",
       " 'conducted',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=['news','editorial','reviews'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus is a convenient resource for studying systematic differences between genres such a **stylistics**.\n",
    "\n",
    "Let's compare genres in their usage of **modal verbs**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist([w.lower() for w in news_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94\n",
      "could: 87\n",
      "may: 93\n",
      "might: 38\n",
      "must: 53\n",
      "will: 389\n"
     ]
    }
   ],
   "source": [
    "for m in modals:\n",
    "    print(m + ':', fdist[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same comparison for six different genres using **conditional frequency distributions** in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genres = ['news','religion','hobbies','science_fiction','romance','humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre,word)\n",
    "                              for genre in brown.categories()\n",
    "                              for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the most frequent modal in \"news\" is **will**, while the most frequent modal in \"romance\" is **could**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### REUTERS CORPUS\n",
    " \n",
    " The Reuters Corpus contains  10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and grouped into two sets, called \"training\" and \"test\".\n",
    " \n",
    " For instance, the text with fileid 'test/14826' is a document drawn from the test set.\n",
    " \n",
    " This split is for training and testing algorithms that automatically detect the topic of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826',\n",
       " 'test/14828',\n",
       " 'test/14829',\n",
       " 'test/14832',\n",
       " 'test/14833',\n",
       " 'test/14839',\n",
       " 'test/14840',\n",
       " 'test/14841',\n",
       " 'test/14842',\n",
       " 'test/14843',\n",
       " 'test/14844',\n",
       " 'test/14849',\n",
       " 'test/14852',\n",
       " 'test/14854',\n",
       " 'test/14858',\n",
       " 'test/14859',\n",
       " 'test/14860',\n",
       " 'test/14861',\n",
       " 'test/14862',\n",
       " 'test/14863',\n",
       " 'test/14865',\n",
       " 'test/14867',\n",
       " 'test/14872',\n",
       " 'test/14873',\n",
       " 'test/14875',\n",
       " 'test/14876',\n",
       " 'test/14877',\n",
       " 'test/14881',\n",
       " 'test/14882',\n",
       " 'test/14885',\n",
       " 'test/14886',\n",
       " 'test/14888',\n",
       " 'test/14890',\n",
       " 'test/14891',\n",
       " 'test/14892',\n",
       " 'test/14899',\n",
       " 'test/14900',\n",
       " 'test/14903',\n",
       " 'test/14904',\n",
       " 'test/14907',\n",
       " 'test/14909',\n",
       " 'test/14911',\n",
       " 'test/14912',\n",
       " 'test/14913',\n",
       " 'test/14918',\n",
       " 'test/14919',\n",
       " 'test/14921',\n",
       " 'test/14922',\n",
       " 'test/14923',\n",
       " 'test/14926',\n",
       " 'test/14928',\n",
       " 'test/14930',\n",
       " 'test/14931',\n",
       " 'test/14932',\n",
       " 'test/14933',\n",
       " 'test/14934',\n",
       " 'test/14941',\n",
       " 'test/14943',\n",
       " 'test/14949',\n",
       " 'test/14951',\n",
       " 'test/14954',\n",
       " 'test/14957',\n",
       " 'test/14958',\n",
       " 'test/14959',\n",
       " 'test/14960',\n",
       " 'test/14962',\n",
       " 'test/14963',\n",
       " 'test/14964',\n",
       " 'test/14965',\n",
       " 'test/14967',\n",
       " 'test/14968',\n",
       " 'test/14969',\n",
       " 'test/14970',\n",
       " 'test/14971',\n",
       " 'test/14974',\n",
       " 'test/14975',\n",
       " 'test/14978',\n",
       " 'test/14981',\n",
       " 'test/14982',\n",
       " 'test/14983',\n",
       " 'test/14984',\n",
       " 'test/14985',\n",
       " 'test/14986',\n",
       " 'test/14987',\n",
       " 'test/14988',\n",
       " 'test/14993',\n",
       " 'test/14995',\n",
       " 'test/14998',\n",
       " 'test/15000',\n",
       " 'test/15001',\n",
       " 'test/15002',\n",
       " 'test/15004',\n",
       " 'test/15005',\n",
       " 'test/15006',\n",
       " 'test/15011',\n",
       " 'test/15012',\n",
       " 'test/15013',\n",
       " 'test/15016',\n",
       " 'test/15017',\n",
       " 'test/15020',\n",
       " 'test/15023',\n",
       " 'test/15024',\n",
       " 'test/15026',\n",
       " 'test/15027',\n",
       " 'test/15028',\n",
       " 'test/15029',\n",
       " 'test/15031',\n",
       " 'test/15032',\n",
       " 'test/15033',\n",
       " 'test/15037',\n",
       " 'test/15038',\n",
       " 'test/15043',\n",
       " 'test/15045',\n",
       " 'test/15046',\n",
       " 'test/15048',\n",
       " 'test/15049',\n",
       " 'test/15052',\n",
       " 'test/15053',\n",
       " 'test/15055',\n",
       " 'test/15056',\n",
       " 'test/15060',\n",
       " 'test/15061',\n",
       " 'test/15062',\n",
       " 'test/15063',\n",
       " 'test/15065',\n",
       " 'test/15067',\n",
       " 'test/15069',\n",
       " 'test/15070',\n",
       " 'test/15074',\n",
       " 'test/15077',\n",
       " 'test/15078',\n",
       " 'test/15079',\n",
       " 'test/15082',\n",
       " 'test/15090',\n",
       " 'test/15091',\n",
       " 'test/15092',\n",
       " 'test/15093',\n",
       " 'test/15094',\n",
       " 'test/15095',\n",
       " 'test/15096',\n",
       " 'test/15097',\n",
       " 'test/15103',\n",
       " 'test/15104',\n",
       " 'test/15106',\n",
       " 'test/15107',\n",
       " 'test/15109',\n",
       " 'test/15110',\n",
       " 'test/15111',\n",
       " 'test/15112',\n",
       " 'test/15118',\n",
       " 'test/15119',\n",
       " 'test/15120',\n",
       " 'test/15121',\n",
       " 'test/15122',\n",
       " 'test/15124',\n",
       " 'test/15126',\n",
       " 'test/15128',\n",
       " 'test/15129',\n",
       " 'test/15130',\n",
       " 'test/15132',\n",
       " 'test/15136',\n",
       " 'test/15138',\n",
       " 'test/15141',\n",
       " 'test/15144',\n",
       " 'test/15145',\n",
       " 'test/15146',\n",
       " 'test/15149',\n",
       " 'test/15152',\n",
       " 'test/15153',\n",
       " 'test/15154',\n",
       " 'test/15156',\n",
       " 'test/15157',\n",
       " 'test/15161',\n",
       " 'test/15162',\n",
       " 'test/15171',\n",
       " 'test/15172',\n",
       " 'test/15175',\n",
       " 'test/15179',\n",
       " 'test/15180',\n",
       " 'test/15185',\n",
       " 'test/15188',\n",
       " 'test/15189',\n",
       " 'test/15190',\n",
       " 'test/15193',\n",
       " 'test/15194',\n",
       " 'test/15197',\n",
       " 'test/15198',\n",
       " 'test/15200',\n",
       " 'test/15204',\n",
       " 'test/15205',\n",
       " 'test/15206',\n",
       " 'test/15207',\n",
       " 'test/15208',\n",
       " 'test/15210',\n",
       " 'test/15211',\n",
       " 'test/15212',\n",
       " 'test/15213',\n",
       " 'test/15217',\n",
       " 'test/15219',\n",
       " 'test/15220',\n",
       " 'test/15221',\n",
       " 'test/15222',\n",
       " 'test/15223',\n",
       " 'test/15226',\n",
       " 'test/15227',\n",
       " 'test/15230',\n",
       " 'test/15233',\n",
       " 'test/15234',\n",
       " 'test/15237',\n",
       " 'test/15238',\n",
       " 'test/15239',\n",
       " 'test/15240',\n",
       " 'test/15242',\n",
       " 'test/15243',\n",
       " 'test/15244',\n",
       " 'test/15246',\n",
       " 'test/15247',\n",
       " 'test/15250',\n",
       " 'test/15253',\n",
       " 'test/15254',\n",
       " 'test/15255',\n",
       " 'test/15258',\n",
       " 'test/15259',\n",
       " 'test/15262',\n",
       " 'test/15263',\n",
       " 'test/15264',\n",
       " 'test/15265',\n",
       " 'test/15270',\n",
       " 'test/15271',\n",
       " 'test/15273',\n",
       " 'test/15274',\n",
       " 'test/15276',\n",
       " 'test/15278',\n",
       " 'test/15280',\n",
       " 'test/15281',\n",
       " 'test/15283',\n",
       " 'test/15287',\n",
       " 'test/15290',\n",
       " 'test/15292',\n",
       " 'test/15294',\n",
       " 'test/15295',\n",
       " 'test/15296',\n",
       " 'test/15299',\n",
       " 'test/15300',\n",
       " 'test/15302',\n",
       " 'test/15303',\n",
       " 'test/15306',\n",
       " 'test/15307',\n",
       " 'test/15308',\n",
       " 'test/15309',\n",
       " 'test/15310',\n",
       " 'test/15311',\n",
       " 'test/15312',\n",
       " 'test/15313',\n",
       " 'test/15314',\n",
       " 'test/15315',\n",
       " 'test/15321',\n",
       " 'test/15322',\n",
       " 'test/15324',\n",
       " 'test/15325',\n",
       " 'test/15326',\n",
       " 'test/15327',\n",
       " 'test/15329',\n",
       " 'test/15335',\n",
       " 'test/15336',\n",
       " 'test/15337',\n",
       " 'test/15339',\n",
       " 'test/15341',\n",
       " 'test/15344',\n",
       " 'test/15345',\n",
       " 'test/15348',\n",
       " 'test/15349',\n",
       " 'test/15351',\n",
       " 'test/15352',\n",
       " 'test/15354',\n",
       " 'test/15356',\n",
       " 'test/15357',\n",
       " 'test/15359',\n",
       " 'test/15363',\n",
       " 'test/15364',\n",
       " 'test/15365',\n",
       " 'test/15366',\n",
       " 'test/15367',\n",
       " 'test/15368',\n",
       " 'test/15372',\n",
       " 'test/15375',\n",
       " 'test/15378',\n",
       " 'test/15379',\n",
       " 'test/15380',\n",
       " 'test/15383',\n",
       " 'test/15384',\n",
       " 'test/15386',\n",
       " 'test/15387',\n",
       " 'test/15388',\n",
       " 'test/15389',\n",
       " 'test/15391',\n",
       " 'test/15394',\n",
       " 'test/15396',\n",
       " 'test/15397',\n",
       " 'test/15400',\n",
       " 'test/15404',\n",
       " 'test/15406',\n",
       " 'test/15409',\n",
       " 'test/15410',\n",
       " 'test/15411',\n",
       " 'test/15413',\n",
       " 'test/15415',\n",
       " 'test/15416',\n",
       " 'test/15417',\n",
       " 'test/15420',\n",
       " 'test/15421',\n",
       " 'test/15424',\n",
       " 'test/15425',\n",
       " 'test/15427',\n",
       " 'test/15428',\n",
       " 'test/15429',\n",
       " 'test/15430',\n",
       " 'test/15431',\n",
       " 'test/15432',\n",
       " 'test/15436',\n",
       " 'test/15438',\n",
       " 'test/15441',\n",
       " 'test/15442',\n",
       " 'test/15444',\n",
       " 'test/15446',\n",
       " 'test/15447',\n",
       " 'test/15448',\n",
       " 'test/15449',\n",
       " 'test/15450',\n",
       " 'test/15451',\n",
       " 'test/15452',\n",
       " 'test/15453',\n",
       " 'test/15454',\n",
       " 'test/15455',\n",
       " 'test/15457',\n",
       " 'test/15459',\n",
       " 'test/15460',\n",
       " 'test/15462',\n",
       " 'test/15464',\n",
       " 'test/15467',\n",
       " 'test/15468',\n",
       " 'test/15471',\n",
       " 'test/15472',\n",
       " 'test/15476',\n",
       " 'test/15477',\n",
       " 'test/15478',\n",
       " 'test/15479',\n",
       " 'test/15481',\n",
       " 'test/15482',\n",
       " 'test/15483',\n",
       " 'test/15484',\n",
       " 'test/15485',\n",
       " 'test/15487',\n",
       " 'test/15489',\n",
       " 'test/15494',\n",
       " 'test/15495',\n",
       " 'test/15496',\n",
       " 'test/15500',\n",
       " 'test/15501',\n",
       " 'test/15503',\n",
       " 'test/15504',\n",
       " 'test/15510',\n",
       " 'test/15511',\n",
       " 'test/15515',\n",
       " 'test/15520',\n",
       " 'test/15521',\n",
       " 'test/15522',\n",
       " 'test/15523',\n",
       " 'test/15527',\n",
       " 'test/15528',\n",
       " 'test/15531',\n",
       " 'test/15532',\n",
       " 'test/15535',\n",
       " 'test/15536',\n",
       " 'test/15539',\n",
       " 'test/15540',\n",
       " 'test/15542',\n",
       " 'test/15543',\n",
       " 'test/15544',\n",
       " 'test/15545',\n",
       " 'test/15547',\n",
       " 'test/15548',\n",
       " 'test/15549',\n",
       " 'test/15550',\n",
       " 'test/15551',\n",
       " 'test/15552',\n",
       " 'test/15553',\n",
       " 'test/15556',\n",
       " 'test/15558',\n",
       " 'test/15559',\n",
       " 'test/15560',\n",
       " 'test/15561',\n",
       " 'test/15562',\n",
       " 'test/15563',\n",
       " 'test/15565',\n",
       " 'test/15566',\n",
       " 'test/15567',\n",
       " 'test/15568',\n",
       " 'test/15569',\n",
       " 'test/15570',\n",
       " 'test/15571',\n",
       " 'test/15572',\n",
       " 'test/15573',\n",
       " 'test/15574',\n",
       " 'test/15575',\n",
       " 'test/15578',\n",
       " 'test/15579',\n",
       " 'test/15580',\n",
       " 'test/15581',\n",
       " 'test/15582',\n",
       " 'test/15583',\n",
       " 'test/15584',\n",
       " 'test/15585',\n",
       " 'test/15590',\n",
       " 'test/15591',\n",
       " 'test/15593',\n",
       " 'test/15594',\n",
       " 'test/15595',\n",
       " 'test/15596',\n",
       " 'test/15597',\n",
       " 'test/15598',\n",
       " 'test/15600',\n",
       " 'test/15601',\n",
       " 'test/15602',\n",
       " 'test/15603',\n",
       " 'test/15605',\n",
       " 'test/15607',\n",
       " 'test/15610',\n",
       " 'test/15613',\n",
       " 'test/15615',\n",
       " 'test/15616',\n",
       " 'test/15617',\n",
       " 'test/15618',\n",
       " 'test/15620',\n",
       " 'test/15621',\n",
       " 'test/15623',\n",
       " 'test/15624',\n",
       " 'test/15625',\n",
       " 'test/15626',\n",
       " 'test/15629',\n",
       " 'test/15632',\n",
       " 'test/15634',\n",
       " 'test/15636',\n",
       " 'test/15637',\n",
       " 'test/15639',\n",
       " 'test/15640',\n",
       " 'test/15641',\n",
       " 'test/15642',\n",
       " 'test/15643',\n",
       " 'test/15646',\n",
       " 'test/15648',\n",
       " 'test/15649',\n",
       " 'test/15651',\n",
       " 'test/15653',\n",
       " 'test/15655',\n",
       " 'test/15656',\n",
       " 'test/15664',\n",
       " 'test/15666',\n",
       " 'test/15667',\n",
       " 'test/15668',\n",
       " 'test/15669',\n",
       " 'test/15672',\n",
       " 'test/15674',\n",
       " 'test/15675',\n",
       " 'test/15676',\n",
       " 'test/15677',\n",
       " 'test/15679',\n",
       " 'test/15680',\n",
       " 'test/15682',\n",
       " 'test/15686',\n",
       " 'test/15688',\n",
       " 'test/15689',\n",
       " 'test/15691',\n",
       " 'test/15692',\n",
       " 'test/15694',\n",
       " 'test/15695',\n",
       " 'test/15696',\n",
       " 'test/15698',\n",
       " 'test/15702',\n",
       " 'test/15703',\n",
       " 'test/15704',\n",
       " 'test/15707',\n",
       " 'test/15708',\n",
       " 'test/15709',\n",
       " 'test/15710',\n",
       " 'test/15713',\n",
       " 'test/15715',\n",
       " 'test/15717',\n",
       " 'test/15719',\n",
       " 'test/15720',\n",
       " 'test/15721',\n",
       " 'test/15723',\n",
       " 'test/15725',\n",
       " 'test/15726',\n",
       " 'test/15727',\n",
       " 'test/15728',\n",
       " 'test/15729',\n",
       " 'test/15732',\n",
       " 'test/15733',\n",
       " 'test/15736',\n",
       " 'test/15737',\n",
       " 'test/15739',\n",
       " 'test/15742',\n",
       " 'test/15749',\n",
       " 'test/15751',\n",
       " 'test/15753',\n",
       " 'test/15757',\n",
       " 'test/15759',\n",
       " 'test/15762',\n",
       " 'test/15767',\n",
       " 'test/15768',\n",
       " 'test/15769',\n",
       " 'test/15772',\n",
       " 'test/15777',\n",
       " 'test/15778',\n",
       " 'test/15780',\n",
       " 'test/15782',\n",
       " 'test/15785',\n",
       " 'test/15790',\n",
       " 'test/15793',\n",
       " 'test/15797',\n",
       " 'test/15798',\n",
       " 'test/15800',\n",
       " 'test/15801',\n",
       " 'test/15803',\n",
       " 'test/15804',\n",
       " 'test/15805',\n",
       " 'test/15807',\n",
       " 'test/15808',\n",
       " 'test/15810',\n",
       " 'test/15811',\n",
       " 'test/15816',\n",
       " 'test/15817',\n",
       " 'test/15819',\n",
       " 'test/15821',\n",
       " 'test/15822',\n",
       " 'test/15823',\n",
       " 'test/15829',\n",
       " 'test/15831',\n",
       " 'test/15832',\n",
       " 'test/15833',\n",
       " 'test/15834',\n",
       " 'test/15836',\n",
       " 'test/15838',\n",
       " 'test/15840',\n",
       " 'test/15841',\n",
       " 'test/15842',\n",
       " 'test/15844',\n",
       " 'test/15845',\n",
       " 'test/15846',\n",
       " 'test/15847',\n",
       " 'test/15851',\n",
       " 'test/15852',\n",
       " 'test/15853',\n",
       " 'test/15854',\n",
       " 'test/15855',\n",
       " 'test/15856',\n",
       " 'test/15858',\n",
       " 'test/15859',\n",
       " 'test/15860',\n",
       " 'test/15861',\n",
       " 'test/15863',\n",
       " 'test/15864',\n",
       " 'test/15865',\n",
       " 'test/15866',\n",
       " 'test/15867',\n",
       " 'test/15868',\n",
       " 'test/15869',\n",
       " 'test/15870',\n",
       " 'test/15871',\n",
       " 'test/15872',\n",
       " 'test/15874',\n",
       " 'test/15875',\n",
       " 'test/15876',\n",
       " 'test/15877',\n",
       " 'test/15878',\n",
       " 'test/15879',\n",
       " 'test/15881',\n",
       " 'test/15885',\n",
       " 'test/15886',\n",
       " 'test/15888',\n",
       " 'test/15889',\n",
       " 'test/15890',\n",
       " 'test/15892',\n",
       " 'test/15893',\n",
       " 'test/15894',\n",
       " 'test/15895',\n",
       " 'test/15896',\n",
       " 'test/15897',\n",
       " 'test/15898',\n",
       " 'test/15899',\n",
       " 'test/15900',\n",
       " 'test/15901',\n",
       " 'test/15902',\n",
       " 'test/15903',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15908',\n",
       " 'test/15909',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15912',\n",
       " 'test/15913',\n",
       " 'test/15914',\n",
       " 'test/15916',\n",
       " 'test/15917',\n",
       " 'test/15918',\n",
       " 'test/15920',\n",
       " 'test/15921',\n",
       " 'test/15922',\n",
       " 'test/15923',\n",
       " 'test/15924',\n",
       " 'test/15925',\n",
       " 'test/15927',\n",
       " 'test/15928',\n",
       " 'test/15929',\n",
       " 'test/15930',\n",
       " 'test/15932',\n",
       " 'test/15933',\n",
       " 'test/15934',\n",
       " 'test/15937',\n",
       " 'test/15939',\n",
       " 'test/15942',\n",
       " 'test/15944',\n",
       " 'test/15949',\n",
       " 'test/15950',\n",
       " 'test/15951',\n",
       " 'test/15952',\n",
       " 'test/15953',\n",
       " 'test/15956',\n",
       " 'test/15959',\n",
       " 'test/15960',\n",
       " 'test/15961',\n",
       " 'test/15963',\n",
       " 'test/15964',\n",
       " 'test/15967',\n",
       " 'test/15968',\n",
       " 'test/15969',\n",
       " 'test/15970',\n",
       " 'test/15973',\n",
       " 'test/15975',\n",
       " 'test/15976',\n",
       " 'test/15977',\n",
       " 'test/15978',\n",
       " 'test/15979',\n",
       " 'test/15980',\n",
       " 'test/15981',\n",
       " 'test/15984',\n",
       " 'test/15985',\n",
       " 'test/15987',\n",
       " 'test/15988',\n",
       " 'test/15989',\n",
       " 'test/15993',\n",
       " 'test/15995',\n",
       " 'test/15996',\n",
       " 'test/15997',\n",
       " 'test/15999',\n",
       " 'test/16002',\n",
       " 'test/16003',\n",
       " 'test/16004',\n",
       " 'test/16005',\n",
       " 'test/16006',\n",
       " 'test/16007',\n",
       " 'test/16009',\n",
       " 'test/16012',\n",
       " 'test/16013',\n",
       " 'test/16014',\n",
       " 'test/16015',\n",
       " 'test/16016',\n",
       " 'test/16021',\n",
       " 'test/16022',\n",
       " 'test/16023',\n",
       " 'test/16026',\n",
       " 'test/16029',\n",
       " 'test/16030',\n",
       " 'test/16033',\n",
       " 'test/16037',\n",
       " 'test/16040',\n",
       " 'test/16041',\n",
       " 'test/16045',\n",
       " 'test/16052',\n",
       " 'test/16053',\n",
       " 'test/16055',\n",
       " 'test/16063',\n",
       " 'test/16066',\n",
       " 'test/16067',\n",
       " 'test/16068',\n",
       " 'test/16069',\n",
       " 'test/16071',\n",
       " 'test/16072',\n",
       " 'test/16074',\n",
       " 'test/16075',\n",
       " 'test/16076',\n",
       " 'test/16077',\n",
       " 'test/16079',\n",
       " 'test/16080',\n",
       " 'test/16083',\n",
       " 'test/16086',\n",
       " 'test/16088',\n",
       " 'test/16091',\n",
       " 'test/16093',\n",
       " 'test/16094',\n",
       " 'test/16095',\n",
       " 'test/16096',\n",
       " 'test/16097',\n",
       " 'test/16098',\n",
       " 'test/16099',\n",
       " 'test/16100',\n",
       " 'test/16103',\n",
       " 'test/16106',\n",
       " 'test/16107',\n",
       " 'test/16108',\n",
       " 'test/16110',\n",
       " 'test/16111',\n",
       " 'test/16112',\n",
       " 'test/16115',\n",
       " 'test/16117',\n",
       " 'test/16118',\n",
       " 'test/16119',\n",
       " 'test/16120',\n",
       " 'test/16122',\n",
       " 'test/16123',\n",
       " 'test/16125',\n",
       " 'test/16126',\n",
       " 'test/16130',\n",
       " 'test/16133',\n",
       " 'test/16134',\n",
       " 'test/16136',\n",
       " 'test/16139',\n",
       " 'test/16140',\n",
       " 'test/16141',\n",
       " 'test/16142',\n",
       " 'test/16143',\n",
       " 'test/16144',\n",
       " 'test/16145',\n",
       " 'test/16146',\n",
       " 'test/16147',\n",
       " 'test/16148',\n",
       " 'test/16149',\n",
       " 'test/16150',\n",
       " 'test/16152',\n",
       " 'test/16155',\n",
       " 'test/16158',\n",
       " 'test/16159',\n",
       " 'test/16161',\n",
       " 'test/16162',\n",
       " 'test/16163',\n",
       " 'test/16164',\n",
       " 'test/16166',\n",
       " 'test/16170',\n",
       " 'test/16171',\n",
       " 'test/16172',\n",
       " 'test/16173',\n",
       " 'test/16175',\n",
       " 'test/16176',\n",
       " 'test/16177',\n",
       " 'test/16179',\n",
       " 'test/16180',\n",
       " 'test/16185',\n",
       " 'test/16188',\n",
       " 'test/16189',\n",
       " 'test/16190',\n",
       " 'test/16193',\n",
       " 'test/16194',\n",
       " 'test/16195',\n",
       " 'test/16196',\n",
       " 'test/16197',\n",
       " 'test/16200',\n",
       " 'test/16201',\n",
       " 'test/16202',\n",
       " 'test/16203',\n",
       " 'test/16206',\n",
       " 'test/16207',\n",
       " 'test/16210',\n",
       " 'test/16211',\n",
       " 'test/16212',\n",
       " 'test/16213',\n",
       " 'test/16214',\n",
       " 'test/16215',\n",
       " 'test/16216',\n",
       " 'test/16219',\n",
       " 'test/16221',\n",
       " 'test/16223',\n",
       " 'test/16225',\n",
       " 'test/16226',\n",
       " 'test/16228',\n",
       " 'test/16230',\n",
       " 'test/16232',\n",
       " 'test/16233',\n",
       " 'test/16234',\n",
       " 'test/16236',\n",
       " 'test/16238',\n",
       " 'test/16241',\n",
       " 'test/16243',\n",
       " 'test/16244',\n",
       " 'test/16246',\n",
       " 'test/16247',\n",
       " 'test/16248',\n",
       " 'test/16250',\n",
       " 'test/16251',\n",
       " 'test/16252',\n",
       " 'test/16255',\n",
       " 'test/16256',\n",
       " 'test/16257',\n",
       " 'test/16258',\n",
       " 'test/16260',\n",
       " 'test/16262',\n",
       " 'test/16263',\n",
       " 'test/16264',\n",
       " 'test/16265',\n",
       " 'test/16266',\n",
       " 'test/16268',\n",
       " 'test/16269',\n",
       " 'test/16270',\n",
       " 'test/16271',\n",
       " 'test/16274',\n",
       " 'test/16275',\n",
       " 'test/16277',\n",
       " 'test/16278',\n",
       " 'test/16279',\n",
       " 'test/16281',\n",
       " 'test/16282',\n",
       " 'test/16283',\n",
       " 'test/16284',\n",
       " 'test/16285',\n",
       " 'test/16286',\n",
       " 'test/16287',\n",
       " 'test/16288',\n",
       " 'test/16289',\n",
       " 'test/16291',\n",
       " 'test/16294',\n",
       " 'test/16297',\n",
       " 'test/16298',\n",
       " 'test/16299',\n",
       " 'test/16300',\n",
       " 'test/16301',\n",
       " 'test/16302',\n",
       " 'test/16303',\n",
       " 'test/16304',\n",
       " 'test/16307',\n",
       " 'test/16310',\n",
       " 'test/16311',\n",
       " 'test/16312',\n",
       " 'test/16314',\n",
       " 'test/16315',\n",
       " 'test/16316',\n",
       " 'test/16317',\n",
       " 'test/16318',\n",
       " 'test/16319',\n",
       " 'test/16320',\n",
       " 'test/16324',\n",
       " 'test/16327',\n",
       " 'test/16331',\n",
       " 'test/16332',\n",
       " 'test/16336',\n",
       " 'test/16337',\n",
       " 'test/16339',\n",
       " 'test/16342',\n",
       " 'test/16343',\n",
       " 'test/16346',\n",
       " 'test/16347',\n",
       " 'test/16348',\n",
       " 'test/16350',\n",
       " 'test/16354',\n",
       " 'test/16357',\n",
       " 'test/16359',\n",
       " 'test/16360',\n",
       " 'test/16362',\n",
       " 'test/16363',\n",
       " 'test/16365',\n",
       " 'test/16366',\n",
       " 'test/16367',\n",
       " 'test/16369',\n",
       " 'test/16370',\n",
       " 'test/16371',\n",
       " 'test/16372',\n",
       " 'test/16374',\n",
       " 'test/16376',\n",
       " 'test/16377',\n",
       " 'test/16379',\n",
       " 'test/16380',\n",
       " 'test/16383',\n",
       " 'test/16385',\n",
       " 'test/16386',\n",
       " 'test/16388',\n",
       " 'test/16390',\n",
       " 'test/16392',\n",
       " 'test/16393',\n",
       " 'test/16394',\n",
       " 'test/16395',\n",
       " 'test/16396',\n",
       " 'test/16398',\n",
       " 'test/16399',\n",
       " 'test/16400',\n",
       " 'test/16401',\n",
       " 'test/16402',\n",
       " 'test/16403',\n",
       " 'test/16404',\n",
       " 'test/16405',\n",
       " 'test/16406',\n",
       " 'test/16407',\n",
       " 'test/16409',\n",
       " 'test/16410',\n",
       " 'test/16415',\n",
       " 'test/16417',\n",
       " 'test/16418',\n",
       " 'test/16419',\n",
       " 'test/16420',\n",
       " 'test/16421',\n",
       " 'test/16422',\n",
       " 'test/16424',\n",
       " 'test/16426',\n",
       " 'test/16427',\n",
       " 'test/16428',\n",
       " 'test/16429',\n",
       " 'test/16430',\n",
       " 'test/16432',\n",
       " 'test/16433',\n",
       " 'test/16434',\n",
       " 'test/16437',\n",
       " 'test/16438',\n",
       " 'test/16440',\n",
       " 'test/16441',\n",
       " 'test/16442',\n",
       " 'test/16443',\n",
       " 'test/16444',\n",
       " 'test/16448',\n",
       " 'test/16449',\n",
       " 'test/16450',\n",
       " 'test/16454',\n",
       " 'test/16457',\n",
       " 'test/16458',\n",
       " 'test/16459',\n",
       " 'test/16460',\n",
       " 'test/16461',\n",
       " 'test/16463',\n",
       " 'test/16465',\n",
       " 'test/16468',\n",
       " 'test/16469',\n",
       " 'test/16470',\n",
       " 'test/16471',\n",
       " 'test/16472',\n",
       " 'test/16473',\n",
       " 'test/16475',\n",
       " 'test/16476',\n",
       " 'test/16478',\n",
       " 'test/16479',\n",
       " 'test/16480',\n",
       " 'test/16481',\n",
       " 'test/16483',\n",
       " 'test/16486',\n",
       " 'test/16487',\n",
       " 'test/16488',\n",
       " 'test/16490',\n",
       " 'test/16492',\n",
       " 'test/16493',\n",
       " 'test/16495',\n",
       " 'test/16496',\n",
       " 'test/16499',\n",
       " 'test/16502',\n",
       " 'test/16505',\n",
       " 'test/16510',\n",
       " 'test/16512',\n",
       " 'test/16513',\n",
       " 'test/16518',\n",
       " 'test/16519',\n",
       " 'test/16521',\n",
       " 'test/16522',\n",
       " 'test/16523',\n",
       " 'test/16525',\n",
       " 'test/16527',\n",
       " 'test/16530',\n",
       " 'test/16531',\n",
       " 'test/16533',\n",
       " 'test/16538',\n",
       " 'test/16539',\n",
       " 'test/16545',\n",
       " 'test/16546',\n",
       " 'test/16549',\n",
       " 'test/16551',\n",
       " 'test/16554',\n",
       " 'test/16555',\n",
       " 'test/16561',\n",
       " 'test/16563',\n",
       " 'test/16564',\n",
       " 'test/16565',\n",
       " 'test/16568',\n",
       " 'test/16569',\n",
       " 'test/16570',\n",
       " 'test/16574',\n",
       " 'test/16577',\n",
       " 'test/16581',\n",
       " 'test/16583',\n",
       " 'test/16584',\n",
       " 'test/16585',\n",
       " 'test/16587',\n",
       " 'test/16588',\n",
       " 'test/16589',\n",
       " 'test/16590',\n",
       " 'test/16591',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus methods accept a single fileid or a list of fileids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'wheat']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'money-fx', 'wheat']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/15618',\n",
       " 'test/15649',\n",
       " 'test/15676',\n",
       " 'test/15728',\n",
       " 'test/15871',\n",
       " 'test/15875',\n",
       " 'test/15952',\n",
       " 'test/17767',\n",
       " 'test/17769',\n",
       " 'test/18024']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.fileids('barley')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the words or sentences we want in terms of files or categories. \n",
    "Remember that titles are store in uppercase by convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH',\n",
       " 'FREE',\n",
       " 'MARKET',\n",
       " 'CEREAL',\n",
       " 'EXPORT',\n",
       " 'BIDS',\n",
       " 'DETAILED',\n",
       " 'French',\n",
       " 'operators',\n",
       " 'have']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/9865')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(categories = 'barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words(categories = ['barley','corn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNOTATED CORPORA\n",
    "\n",
    "Many text corpora contain linguistic annotations, representing part of the speech tags, named entities, syntatic structures, semantic roles, and so forth.\n",
    "For information on how to download free corpora, samples and data packages check http://nltk.org/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPORA IN OTHER LANGUAGES\n",
    "\n",
    "NLTK comes with corpora for many languages, though in some cases you will need to learn how to manipulate character encodings in Python before using these corpora.\n",
    "\n",
    "Let's use corpora from the UNiversal Declaration of Human Rights in over 300 languages and create a conditional Frquency distribution to examine the differences in words lengths for a group od languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['Chickasaw', 'English','German_Deutsch', 'Greenlandic_Inuktikut',\n",
    "             'Hungarian_Magyar', 'Ibibio_Efik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((lang, len(word))\n",
    "                              for lang in languages\n",
    "                              for word in udhr.words(lang + '-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING YOUR OWN CORPUS\n",
    "\n",
    "If you have your own collection of text files, you can load them using NLTK's PlaintextCorpusReader.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Set the value of corpus_root to the location of your files\n",
    "The second paramenter of the PlaintextCorpusReader initializer can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern that matches all the fileids, like '[abc]/.*\\.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = '/Users/dianaamador/Corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordlists = PlaintextCorpusReader(corpus_root,'.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'Im_the_shooter_HP.txt', 'Orlando_NYT.txt', 'UTF.txt']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTF', '-', '8', 'is', 'a', 'character', 'encoding', ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.words('UTF.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UTF', '-', '8', 'is', 'a', 'character', 'encoding', 'capable', 'of', 'encoding', 'all', 'possible', 'characters', ',', 'or', 'code', 'points', ',', 'defined', 'by', 'Unicode', 'and', 'originally', 'designed', 'by', 'Ken', 'Thompson', 'and', 'Rob', 'Pike', '.'], ['The', 'encoding', 'is', 'variable', '-', 'length', 'and', 'uses', '8', '-', 'bit', 'code', 'units', '.']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.sents('UTF.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘', 'I', '’', 'm', 'the', 'shooter', '.', 'It', '’', ...]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlists.words('Im_the_shooter_HP.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlists.sents('Im_the_shooter_HP.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Corpus Functionality defined in NLTK (http://www.nltk,org/howto)\n",
    "\n",
    "fileids() The files of the corpus   \n",
    "fileids([categories]) The files of the corpus corresponding to these categories    \n",
    "categories() The categories of the corpus   \n",
    "categories([fileids]) The categories of the corpus corresponding to these files   \n",
    "raw() The raw content of the corpus   \n",
    "raw(fileids=[f1,f2,f3]) The raw content of the specified files   \n",
    "raw(categories=[c1,c2]) The raw content of the specified categories   \n",
    "words() The words of the whole corpus   \n",
    "words(fileids=[f1,f2,f3]) The words of the specified fileids   \n",
    "words(categories=[c1,c2]) The words of the specified categories   \n",
    "sents() The sentences of the specified categories   \n",
    "sents(fileids=[f1,f2,f3]) The sentences of the specified fileids    \n",
    "sents(categories=[c1,c2]) The sentences of the specified categories    \n",
    "abspath(fileid) The location of the given file on disk   \n",
    "encoding(fileid) The encoding of the file (if known)   \n",
    "open(fileid) Open a stream for reading the given corpus file    \n",
    "root() The path to the root of locally installed corpus    \n",
    "readme() The contents of the README file of the corpus   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
